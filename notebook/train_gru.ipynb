{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ivgAZqmyUpA6"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import lightgbm as lgb\n",
        "from typing import Tuple, List, Dict\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "from tqdm import tqdm\n",
        "from polars.dataframe.group_by import GroupBy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    Flatten,\n",
        "    Concatenate,\n",
        "    GaussianNoise,\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.experimental import CosineDecay\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.layers import concatenate, Dropout\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import random\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Embedding,\n",
        "    Lambda,\n",
        "    Reshape,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    BatchNormalization,\n",
        "    Dropout,\n",
        "    concatenate,\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import ZeroPadding1D\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "\n",
        "from tensorflow.keras.layers import (\n",
        "    Input,\n",
        "    Embedding,\n",
        "    Lambda,\n",
        "    Reshape,\n",
        "    LSTM,\n",
        "    Dense,\n",
        "    BatchNormalization,\n",
        "    Dropout,\n",
        "    concatenate,\n",
        ")\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import ZeroPadding1D, Activation\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import MaxPooling1D, AveragePooling1D\n",
        "from tensorflow.keras.layers import Add\n",
        "\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rxkoGJJwU7dp"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "class Runtime(Enum):\n",
        "    LOCAL = 0\n",
        "    COLAB = 1\n",
        "    KAGGLE = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a4SLykOPXb9X"
      },
      "outputs": [],
      "source": [
        "runtime = Runtime.COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_irr6bHkXWtP",
        "outputId": "4ec2d941-76f7-442f-f3ae-e7f041feeb7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if runtime == Runtime.COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jKDekshqXhib"
      },
      "outputs": [],
      "source": [
        "dates_train = (0, 400)\n",
        "dates_test = (401, 480)\n",
        "num_models = {\"rnn\": 1}\n",
        "models_path = \"gru_models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cpTPrTS4XmOo"
      },
      "outputs": [],
      "source": [
        "rnn_ep = 100\n",
        "rnn_lr = 0.001\n",
        "rnn_bs = 2**12\n",
        "window_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96RSveYgXnOn"
      },
      "outputs": [],
      "source": [
        "def split_by_date(df: pl.DataFrame, dates: Tuple[int, int]) -> pl.DataFrame:\n",
        "    return df.filter(\n",
        "        pl.col(\"date_id\").ge(dates[0]).and_(pl.col(\"date_id\").le(dates[1]))\n",
        "    )\n",
        "\n",
        "\n",
        "def make_predictions(models, X_test, model=\"nn\"):\n",
        "    if model == \"nn\":\n",
        "        all_predictions = [model.predict(X_test, batch_size=16384) for model in models]\n",
        "    if model == \"lgb\" or model == \"xgb\" or model == \"cat\":\n",
        "        all_predictions = [model.predict(X_test) for model in models]\n",
        "    prediction = np.mean(all_predictions, axis=0)\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vvQ_fUCEXoKl"
      },
      "outputs": [],
      "source": [
        "# @title TPU\n",
        "try:\n",
        "    # Create a TPUClusterResolver\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    # Connect to the TPU cluster\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    # Initialize the TPU system\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    # Create a TPUStrategy for distributed training\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    tpu_strategy = None  # No TPU found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "82tWZ7d6lMjM"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import List\n",
        "from itertools import combinations\n",
        "from typing import Tuple\n",
        "\n",
        "# @title Feature Engineering Functions\n",
        "\n",
        "\n",
        "def lag_function(\n",
        "    df: pl.DataFrame, columns_to_lag: List[str], num_days_to_lag: List[int]\n",
        ") -> pl.DataFrame:\n",
        "    cols = [\n",
        "        pl.col(columns_to_lag)\n",
        "        .shift(i)\n",
        "        .over([\"stock_id\", \"seconds_in_bucket\"])\n",
        "        .name.prefix(f\"lag{i}_\")\n",
        "        for i in num_days_to_lag\n",
        "    ]\n",
        "    return df.with_columns(*cols)\n",
        "\n",
        "\n",
        "def create_diff_lagged_features_within_date(\n",
        "    df: pl.DataFrame, columns_to_lag: List[str], lags: List[int]\n",
        ") -> pl.DataFrame:\n",
        "    cols = [\n",
        "        pl.col(columns_to_lag)\n",
        "        .sub(pl.col(columns_to_lag).shift(lag).over([\"stock_id\", \"date_id\"]))\n",
        "        .name.suffix(f\"_lag_{lag}\")\n",
        "        for lag in lags\n",
        "    ]\n",
        "    return df.with_columns(*cols)\n",
        "\n",
        "\n",
        "def create_features_from_start(df: pl.DataFrame, columns: List[str]) -> pl.DataFrame:\n",
        "    return df.with_columns(\n",
        "        pl.col(columns)\n",
        "        .sub(pl.col(columns).first().over([\"stock_id\", \"date_id\"]))\n",
        "        .name.suffix(\"_from_start\")\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_imbalances(df: pl.DataFrame, columns: List[str], prefix=\"\") -> pl.DataFrame:\n",
        "    cols = []\n",
        "    for col1, col2 in combinations(columns, 2):\n",
        "        col1, col2 = sorted([col1, col2])\n",
        "        total = pl.col(col1).add(pl.col(col2))\n",
        "        col_name = f\"{col1}_{col2}_imbalance_{prefix}\"\n",
        "        cols.append(pl.col(col1).sub(pl.col(col2)).truediv(total).alias(col_name))\n",
        "    return df.with_columns(*cols)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F66oKiUWk-Xv"
      },
      "outputs": [],
      "source": [
        "#@title pipeline\n",
        "\n",
        "raw_cols          = ['imbalance_size','matched_size','bid_size','ask_size','reference_price','far_price','near_price','bid_price','ask_price','wap','imbalance_buy_sell_flag']\n",
        "\n",
        "columns_prices    = ['reference_price','far_price','near_price','bid_price','ask_price','wap']\n",
        "columns_4prices   = ['reference_price','bid_price','ask_price','wap']\n",
        "\n",
        "columns_sizes     = ['imbalance_size','matched_size','bid_size','ask_size']\n",
        "columns_flag      = ['imbalance_buy_sell_flag']\n",
        "\n",
        "\n",
        "num_of_target_lags  = 3\n",
        "target_lags         = list(range(1,num_of_target_lags+1))\n",
        "\n",
        "def feature_pipeline(df: pl.DataFrame):\n",
        "\n",
        "    df = compute_imbalances(df, columns_sizes, prefix='_sz_')\n",
        "    df = compute_imbalances(df, columns_prices, prefix = '_pr_')\n",
        "\n",
        "\n",
        "    print(f\"lagging target column for {len(target_lags)} lags.\")\n",
        "    df = lag_function(df, ['target'], target_lags)\n",
        "\n",
        "    print(\"Done...\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X9s_hmuRXqA9"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "from numpy import dtype, ndarray\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "class BestScoresCallback(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            train_loss = logs.get('loss', float('inf'))\n",
        "            val_loss = logs.get('val_loss', float('inf'))\n",
        "\n",
        "            if train_loss < self.best_train_loss:\n",
        "                self.best_train_loss = train_loss\n",
        "            if val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        print(f\"Best training loss: {self.best_train_loss}, Best validation loss: {self.best_val_loss}\")\n",
        "\n",
        "# @title RNN second pass\n",
        "\n",
        "\n",
        "def precompute_sequences(\n",
        "    stock_data: pl.DataFrame,\n",
        "    window_size: int,\n",
        "    rnn_numerical_features: List[str],\n",
        "    rnn_categorical_features: List[str],\n",
        ") -> Tuple[ndarray[Any, dtype[Any]], ndarray[Any, Any]]:\n",
        "    # Convert DataFrame columns to NumPy arrays\n",
        "    stock_data_num = stock_data.select(rnn_numerical_features).to_numpy()\n",
        "    stock_data_cat = stock_data.select(rnn_categorical_features).to_numpy()\n",
        "\n",
        "    # Pre-compute all sequences\n",
        "    all_sequences_num = [\n",
        "        stock_data_num[max(0, i - window_size + 1) : i + 1]\n",
        "        for i in range(len(stock_data))\n",
        "    ]\n",
        "    all_sequences_cat = [\n",
        "        stock_data_cat[max(0, i - window_size + 1) : i + 1]\n",
        "        for i in range(len(stock_data))\n",
        "    ]\n",
        "\n",
        "    # Add padding if necessary\n",
        "    padded_sequences_num = [\n",
        "        np.pad(seq, ((window_size - len(seq), 0), (0, 0)), \"constant\")\n",
        "        for seq in all_sequences_num\n",
        "    ]\n",
        "    padded_sequences_cat = [\n",
        "        np.pad(seq, ((window_size - len(seq), 0), (0, 0)), \"constant\")\n",
        "        for seq in all_sequences_cat\n",
        "    ]\n",
        "\n",
        "    # Combine numerical and categorical features\n",
        "    combined_sequences = np.array(\n",
        "        [\n",
        "            np.concatenate([num, cat], axis=-1)\n",
        "            for num, cat in zip(padded_sequences_num, padded_sequences_cat)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extract targets\n",
        "    targets = stock_data.select(\"target\").to_numpy()\n",
        "\n",
        "    return combined_sequences, targets\n",
        "\n",
        "\n",
        "def get_sequence(precomputed_data: Tuple[NDArray, NDArray], time_step):\n",
        "    combined_sequences, targets = precomputed_data\n",
        "    return combined_sequences[time_step], targets[time_step]\n",
        "\n",
        "\n",
        "def create_batches(\n",
        "    data: pl.DataFrame,\n",
        "    window_size: int,\n",
        "    rnn_numerical_features: List[str],\n",
        "    rnn_categorical_features: List[str],\n",
        "    max_time_steps: int = 55,\n",
        "):\n",
        "\n",
        "    grouped = data.group_by([\"stock_id\", \"date_id\"], maintain_order=True)\n",
        "    all_batches = []\n",
        "    all_targets = []\n",
        "\n",
        "    for _, group in tqdm(grouped, desc=\"Processing groups\"):\n",
        "        # Precompute sequences for the current group\n",
        "        precomputed_data = precompute_sequences(\n",
        "            group, window_size, rnn_numerical_features, rnn_categorical_features\n",
        "        )\n",
        "\n",
        "        # Initialize containers for group sequences and targets\n",
        "        group_sequences = []\n",
        "        group_targets = []\n",
        "\n",
        "        # Iterate over the time steps and retrieve precomputed sequences\n",
        "        for time_step in range(max_time_steps):\n",
        "            sequence, target = get_sequence(precomputed_data, time_step)\n",
        "            if sequence.size > 0:\n",
        "                group_sequences.append(sequence)\n",
        "                group_targets.append(target)\n",
        "\n",
        "        # Extend the main batches with the group's sequences and targets\n",
        "        all_batches.extend(group_sequences)\n",
        "        all_targets.extend(group_targets)\n",
        "\n",
        "    return all_batches, all_targets\n",
        "\n",
        "\n",
        "def compute_last_sequence(\n",
        "    group: pl.DataFrame, window_size: int, rnn_numerical_features: List[str], rnn_categorical_features: List[str]\n",
        "):\n",
        "    # Convert DataFrame columns to NumPy arrays\n",
        "    # group_all = group.all()\n",
        "    stock_data_num = group.select(rnn_numerical_features).to_numpy()\n",
        "    stock_data_cat = group.select(rnn_categorical_features).to_numpy()\n",
        "    stock_data_target = group.select(\"target\").to_numpy()\n",
        "\n",
        "    # Find the index of the target second\n",
        "    target_index = len(group) - 1\n",
        "\n",
        "    # Extract the sequence for the target index\n",
        "    sequence_num = stock_data_num[\n",
        "        max(0, target_index - window_size + 1) : target_index + 1\n",
        "    ]\n",
        "    sequence_cat = stock_data_cat[\n",
        "        max(0, target_index - window_size + 1) : target_index + 1\n",
        "    ]\n",
        "\n",
        "    # Add padding if necessary\n",
        "    padded_sequence_num = np.pad(\n",
        "        sequence_num, ((window_size - len(sequence_num), 0), (0, 0)), \"constant\"\n",
        "    )\n",
        "    padded_sequence_cat = np.pad(\n",
        "        sequence_cat, ((window_size - len(sequence_cat), 0), (0, 0)), \"constant\"\n",
        "    )\n",
        "\n",
        "    # Combine numerical and categorical features\n",
        "    combined_sequence = np.concatenate(\n",
        "        [padded_sequence_num, padded_sequence_cat], axis=-1\n",
        "    )\n",
        "\n",
        "    # Extract target\n",
        "    target = stock_data_target[-1]\n",
        "\n",
        "    return combined_sequence, target\n",
        "\n",
        "\n",
        "def create_last_batches(\n",
        "    data: pl.DataFrame, window_size: int, rnn_numerical_features: List[str], rnn_categorical_features: List[str]\n",
        "):\n",
        "\n",
        "    grouped = data.group_by([\"stock_id\"], maintain_order=True)\n",
        "    all_batches = []\n",
        "    all_targets = []\n",
        "\n",
        "    for _, group in grouped:\n",
        "        # Compute the sequence for the last data point in the current group\n",
        "        last_sequence, last_target = compute_last_sequence(\n",
        "            group, window_size, rnn_numerical_features, rnn_categorical_features\n",
        "        )\n",
        "\n",
        "        # Check if the sequence is valid (i.e., not empty)\n",
        "        if last_sequence.size > 0:\n",
        "            all_batches.append(last_sequence)\n",
        "            all_targets.append(last_target)\n",
        "\n",
        "    return all_batches, all_targets\n",
        "\n",
        "\n",
        "def second_pass_for_rnn(\n",
        "    df: pl.DataFrame,\n",
        "    rnn_numerical_features: List[str],\n",
        "    rnn_categorical_features: List[str],\n",
        "    window_size: int,\n",
        "    rnn_scaler: StandardScaler,\n",
        "    rnn_medians: Dict[str, float],\n",
        "    is_inference=False,\n",
        "):\n",
        "\n",
        "    # Standard scaling for numerical features\n",
        "    if is_inference:\n",
        "        df = df.with_columns(\n",
        "            [pl.col(col).fill_null(rnn_medians[col]) for col in rnn_numerical_features]\n",
        "        )\n",
        "        df = df.with_columns(\n",
        "            [\n",
        "                pl.Series(\n",
        "                    rnn_scaler.fit_transform(df.select(pl.col(col)).to_numpy()).flatten()\n",
        "                ).alias(col)\n",
        "                for col in rnn_numerical_features\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Preprocess Data\n",
        "    df = df.with_columns(\n",
        "        [\n",
        "            pl.col(\"seconds_in_bucket\").truediv(10).alias(\"seconds_in_bucket\"),\n",
        "            pl.col(\"imbalance_buy_sell_flag\").add(1).alias(\"imbalance_buy_sell_flag\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    if is_inference:\n",
        "        df_copy_batches, df_copy_targets = create_last_batches(\n",
        "            df, window_size, rnn_numerical_features, rnn_categorical_features\n",
        "        )\n",
        "    else:\n",
        "        df_copy_batches, df_copy_targets = create_batches(\n",
        "            df, window_size, rnn_numerical_features, rnn_categorical_features\n",
        "        )\n",
        "\n",
        "    del df\n",
        "    import gc\n",
        "    gc.collect()\n",
        "\n",
        "    df_copy_batches = np.array(df_copy_batches)\n",
        "    df_copy_targets = np.array(df_copy_targets)\n",
        "\n",
        "    return df_copy_batches, df_copy_targets\n",
        "\n",
        "\n",
        "# ------------------------------------------------- NN Second Pass Functions -----------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-RC55ku0Xs4V"
      },
      "outputs": [],
      "source": [
        "#@title RNN model\n",
        "from tensorflow.keras.layers import Input, Embedding, Lambda, Reshape, LSTM, GRU, Dense, BatchNormalization, Dropout, concatenate\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import ZeroPadding1D\n",
        "\n",
        "\n",
        "def create_rnn_model_with_residual(window_size, numerical_features, initial_learning_rate=0.001):\n",
        "\n",
        "    categorical_features = 'seconds_in_bucket'\n",
        "    categorical_uniques  = { 'seconds_in_bucket' : 55}\n",
        "    embedding_dim        = {'seconds_in_bucket' : 10}\n",
        "\n",
        "    input_layer = Input(shape=(window_size, len(numerical_features) + 1), name=\"combined_input\")\n",
        "\n",
        "    # Split the input into numerical and categorical parts\n",
        "    numerical_input = Lambda(lambda x: x[:, :, :-1], name=\"numerical_part\")(input_layer)\n",
        "    categorical_input = Lambda(lambda x: x[:, :, -1:], name=\"categorical_part\")(input_layer)\n",
        "\n",
        "    # Function to create a difference layer for a given lag\n",
        "    def create_difference_layer(lag):\n",
        "        return Lambda(lambda x: x[:, lag:, :] - x[:, :-lag, :], name=f\"difference_layer_lag{lag}\")\n",
        "\n",
        "    # List to store all difference layers\n",
        "    difference_layers = []\n",
        "\n",
        "    # Create difference layers for each lag\n",
        "    for lag in range(1, window_size):\n",
        "        diff_layer = create_difference_layer(lag)(numerical_input)\n",
        "        padding = ZeroPadding1D(padding=(lag, 0))(diff_layer)  # Add padding to the beginning of the sequence\n",
        "        difference_layers.append(padding)\n",
        "\n",
        "\n",
        "\n",
        "    combined_diff_layer = Concatenate(name=\"combined_difference_layer\")(difference_layers)\n",
        "\n",
        "    enhanced_numerical_input = Concatenate(name=\"enhanced_numerical_input\")([numerical_input, combined_diff_layer])\n",
        "\n",
        "#     concat_input = Concatenate(name=\"concatenated_input\")([enhanced_numerical_input, categorical_input])\n",
        "\n",
        "    # Embedding for categorical part\n",
        "    vocab_size, embedding_dim = categorical_uniques[categorical_features], embedding_dim[categorical_features]\n",
        "    embedding = Embedding(vocab_size, embedding_dim, input_length=window_size)(categorical_input)\n",
        "    embedding = Reshape((window_size, -1))(embedding)\n",
        "\n",
        "\n",
        "\n",
        "    # Concatenate numerical input and embedding\n",
        "    gru_input = concatenate([enhanced_numerical_input, embedding], axis=-1)\n",
        "\n",
        "    # Initialize a list to hold the outputs of each LSTM layer\n",
        "#     lstm_outputs = []\n",
        "\n",
        "    # First LSTM layer\n",
        "    grul = GRU(64, return_sequences=False)(gru_input)\n",
        "    grul = BatchNormalization()(grul)\n",
        "    grul = Dropout(0.3)(grul)\n",
        "\n",
        "    dense_output = grul\n",
        "    dense_sizes = [512, 256, 128, 64, 32]\n",
        "    do_ratio = 0.3\n",
        "    for size in dense_sizes:\n",
        "        dense_output = Dense(size, activation='swish')(dense_output)\n",
        "        dense_output = BatchNormalization()(dense_output)\n",
        "        dense_output = Dropout(do_ratio)(dense_output)\n",
        "\n",
        "    # Output layer\n",
        "    output = Dense(1, name='output_layer')(dense_output)\n",
        "\n",
        "    # Learning rate schedule\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=10000,\n",
        "        decay_rate=0.7,\n",
        "        staircase=True)\n",
        "\n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=input_layer, outputs=output)\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=\"mean_absolute_error\")\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIkS8lwrXxM_",
        "outputId": "f93b4a47-3ed9-43f6-af26-9c1d5540b1c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we have 87 features\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing groups: 79236it [07:25, 177.98it/s]\n",
            "Processing groups: 16000it [01:28, 180.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train batches shape:(4357980, 3, 87)\n"
          ]
        }
      ],
      "source": [
        "excluded_columns = [\n",
        "    \"row_id\",\n",
        "    \"date_id\",\n",
        "    \"time_id\",\n",
        "    \"target\",\n",
        "    \"stock_return\",\n",
        "    \"stock_id\",\n",
        "]\n",
        "\n",
        "match runtime:\n",
        "    case Runtime.LOCAL:\n",
        "        root_dir = \".\"\n",
        "    case Runtime.COLAB:\n",
        "        root_dir = \"/content/drive/MyDrive/optiver\"\n",
        "    case Runtime.KAGGLE:\n",
        "        root_dir = \"/kaggle/input/optiver-train\"\n",
        "\n",
        "train_eng = pl.read_parquet(f\"{root_dir}/data/train_eng_rnn.parquet\")\n",
        "features = [col for col in train_eng.schema.keys() if col not in excluded_columns]\n",
        "categorical_features = [\"seconds_in_bucket\"]\n",
        "numerical_features = [\n",
        "    col for col in features if col not in categorical_features\n",
        "]\n",
        "print(\"we have {} features\".format(len(features)))\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "medians = train_eng.select([pl.col(f).median() for f in numerical_features]).to_dict(\n",
        "    as_series=False\n",
        ")\n",
        "medians = {k: v[0] for k, v in medians.items()}\n",
        "\n",
        "\n",
        "train_eng = train_eng.with_columns(\n",
        "    [pl.col(f).fill_null(medians[f]) for f in numerical_features]\n",
        ")\n",
        "\n",
        "train_eng = train_eng.with_columns(\n",
        "    [pl.when(pl.col(f).is_nan()).then(medians[f]).otherwise(pl.col(f)).alias(f) for f in numerical_features]\n",
        ")\n",
        "\n",
        "train_eng = train_eng.with_columns(\n",
        "    [\n",
        "        pl.Series(\n",
        "            scaler.fit_transform(train_eng.select(pl.col(f)).to_numpy()).flatten()\n",
        "        ).alias(f)\n",
        "        for f in numerical_features\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "train_data = split_by_date(train_eng, dates_train)\n",
        "test_data = split_by_date(train_eng, dates_test)\n",
        "stock_ids = test_data.select(\"stock_id\").to_numpy().flatten()\n",
        "\n",
        "\n",
        "del train_eng\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "train_batches, train_targets = second_pass_for_rnn(\n",
        "    train_data,\n",
        "    numerical_features,\n",
        "    categorical_features,\n",
        "    window_size,\n",
        "    rnn_scaler=scaler,\n",
        "    rnn_medians=medians,\n",
        ")\n",
        "test_batches, test_targets = second_pass_for_rnn(\n",
        "    test_data,\n",
        "    numerical_features,\n",
        "    categorical_features,\n",
        "    window_size,\n",
        "    rnn_scaler=scaler,\n",
        "    rnn_medians=medians,\n",
        ")\n",
        "print(f\"train batches shape:{train_batches.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30G9yf-qX7TM",
        "outputId": "2f5067ef-3c43-4c04-bd68-799bfded1172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training rnn model 1 out of 1 with seed 42\n",
            "---------------------------------------\n",
            "Epoch 1/100\n",
            "1064/1064 [==============================] - 27s 17ms/step - loss: 6.3853 - val_loss: 5.9022\n",
            "Epoch 2/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3516 - val_loss: 5.8858\n",
            "Epoch 3/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3389 - val_loss: 5.8801\n",
            "Epoch 4/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3304 - val_loss: 5.8751\n",
            "Epoch 5/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3244 - val_loss: 5.8730\n",
            "Epoch 6/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3201 - val_loss: 5.8684\n",
            "Epoch 7/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3164 - val_loss: 5.8696\n",
            "Epoch 8/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3134 - val_loss: 5.8707\n",
            "Epoch 9/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3100 - val_loss: 5.8712\n",
            "Epoch 10/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3062 - val_loss: 5.8650\n",
            "Epoch 11/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.3023 - val_loss: 5.8678\n",
            "Epoch 12/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2998 - val_loss: 5.8660\n",
            "Epoch 13/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2986 - val_loss: 5.8657\n",
            "Epoch 14/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2966 - val_loss: 5.8664\n",
            "Epoch 15/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2941 - val_loss: 5.8656\n",
            "Epoch 16/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2930 - val_loss: 5.8654\n",
            "Epoch 17/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2918 - val_loss: 5.8670\n",
            "Epoch 18/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2898 - val_loss: 5.8667\n",
            "Epoch 19/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2883 - val_loss: 5.8674\n",
            "Epoch 20/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2849 - val_loss: 5.8681\n",
            "Epoch 21/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2841 - val_loss: 5.8662\n",
            "Epoch 22/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2829 - val_loss: 5.8666\n",
            "Epoch 23/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2817 - val_loss: 5.8662\n",
            "Epoch 24/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2810 - val_loss: 5.8663\n",
            "Epoch 25/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2793 - val_loss: 5.8673\n",
            "Epoch 26/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2789 - val_loss: 5.8667\n",
            "Epoch 27/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2783 - val_loss: 5.8667\n",
            "Epoch 28/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2772 - val_loss: 5.8674\n",
            "Epoch 29/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2747 - val_loss: 5.8662\n",
            "Epoch 30/100\n",
            "1064/1064 [==============================] - 16s 15ms/step - loss: 6.2728 - val_loss: 5.8687\n",
            "---------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 1s 16ms/step\n",
            "Ensemble Mean Absolute Error: 5.86499\n",
            "GRU Ensemble + PP Mean Absolute Error: 5.86509\n"
          ]
        }
      ],
      "source": [
        "log_dir = f\"{root_dir}/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "# callbacks = [BestScoresCallback()]  # Always include BestScoresCallback\n",
        "callbacks = []\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=False)\n",
        "callbacks.append(early_stopping)\n",
        "\n",
        "if dates_train[1] != 480:\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=20, restore_best_weights=True\n",
        "    )\n",
        "    callbacks.append(early_stopping)\n",
        "    from keras.callbacks import ModelCheckpoint\n",
        "    model_checkpoint_callback = ModelCheckpoint(\n",
        "        filepath=f\"{root_dir}/{models_path}/checkpoint.model.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "    )\n",
        "    callbacks.append(model_checkpoint_callback)\n",
        "\n",
        "os.makedirs(f\"{root_dir}/{models_path}\", exist_ok=True)\n",
        "rnn_models = []\n",
        "for i in range(num_models[\"rnn\"]):\n",
        "\n",
        "    print(f\"Training rnn model {i+1} out of {num_models['rnn']} with seed {42+i}\")\n",
        "    print(\"---------------------------------------\")\n",
        "    set_all_seeds(42 + i)\n",
        "\n",
        "    rnn_model = create_rnn_model_with_residual(\n",
        "        window_size, numerical_features, initial_learning_rate=rnn_lr\n",
        "    )\n",
        "\n",
        "    from tensorflow.keras.utils import plot_model\n",
        "    plot_model(rnn_model, to_file='gru_model.png', show_shapes=True, show_layer_names=True, dpi=96)\n",
        "\n",
        "    history = rnn_model.fit(\n",
        "        train_batches,\n",
        "        train_targets,\n",
        "        validation_data=(test_batches, test_targets),\n",
        "        epochs=rnn_ep,\n",
        "        batch_size=rnn_bs,\n",
        "        callbacks=callbacks,\n",
        "    )\n",
        "    print(\"---------------------------------------\")\n",
        "    rnn_model.save(f\"{root_dir}/{models_path}/rnn_model_seed_{i}.h5\")\n",
        "    rnn_models.append(rnn_model)\n",
        "\n",
        "# rnn_model.load_model(f\"{root_dir}/{models_path}/checkpoint.model.keras\")\n",
        "predictions = make_predictions(rnn_models, test_batches, model=\"nn\")\n",
        "print(\n",
        "    f\"Ensemble Mean Absolute Error: {mean_absolute_error(test_targets, predictions):.5f}\"\n",
        ")\n",
        "\n",
        "prediction_df = pd.DataFrame(\n",
        "    {\n",
        "        \"stock_id\": stock_ids,\n",
        "        \"target\": predictions.flatten(),\n",
        "    }\n",
        ")\n",
        "weight = json.load(open(f\"{root_dir}/data/weight.json\"))\n",
        "weight = dict(zip(range(200), weight))\n",
        "\n",
        "prediction_df[\"stock_weights\"] = prediction_df[\"stock_id\"].map(weight)\n",
        "prediction_df[\"target\"] = (\n",
        "    prediction_df[\"target\"]\n",
        "    - (prediction_df[\"target\"] * prediction_df[\"stock_weights\"]).sum()\n",
        "    / prediction_df[\"stock_weights\"].sum()\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"GRU Ensemble + PP Mean Absolute Error: {mean_absolute_error(test_targets, prediction_df['target']):.5f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N6PPc9nl4HX2"
      },
      "outputs": [],
      "source": [
        "prediction_df.to_parquet(f\"{root_dir}/data/prediction_gru.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NwZ98pqWo1O7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
